{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ibrahim-Ayaz/Agriculture-Computer-Vision-Object-Detection-Dataset/blob/main/agriculture_computer_vision_object_detection_end_to_end_projectipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YAGItmhcRpb"
      },
      "source": [
        "# Agriculture End-to-End Computer Vision Object Detection Project\n",
        "\n",
        "This project engineers a full-stack, production-oriented computer vision pipeline for agricultural object detection, designed to operate on real-world field imagery under practical accuracy–performance constraints. The system integrates dataset ingestion, deterministic train/validation/test partitioning, large-scale model training, and batch inference into a reproducible workflow. A high-capacity transformer-based detector (RT-DETR-L, PyTorch) is optimised and validated using industry-standard detection metrics, including mAP, per-class precision and recall, IoU-based localisation analysis, and detection-aware confusion matrices. Beyond aggregate scores, the pipeline incorporates class-level error analysis and qualitative inspection to identify failure modes, assess model robustness, and validate suitability for deployment in applied agricultural monitoring and decision-support systems.\n",
        "\n",
        "* For PyTorch, check out the following link: https://docs.pytorch.org/docs/stable/index.html\n",
        "\n",
        "* For more about the model, check out the `torchvision` manual: https://docs.pytorch.org/vision/0.9/models.html\n",
        "\n",
        "* For the metrics, check them out here: https://docs.pytorch.org/docs/stable/elastic/metrics.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrkpWVowkO0U"
      },
      "source": [
        "## Confirming access to a GPU\n",
        "\n",
        "GPU availability is a prerequisite for running large-scale transformer detectors and for achieving feasible performance under real-world engineering constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOS23WN4cOAm"
      },
      "outputs": [],
      "source": [
        "# Confirm GPU access\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFWM1zz8kjXR"
      },
      "source": [
        "## Installing required dependencies for our project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82c9vbWWrLa2"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "import os, shutil, glob, random\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Setup random seed for reproducibility\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfmtg5sMrPog"
      },
      "outputs": [],
      "source": [
        "# Mount content dir to Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjAOhvoKkqKe"
      },
      "source": [
        "## Analysing each of our dir splits\n",
        "\n",
        "Validates train/validation/test directory splits for structural integrity, label alignment, and class distribution consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw4ZnKVLrQtn"
      },
      "outputs": [],
      "source": [
        "# Inspect the contents (everything) in the target Drive directory\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive/DataSet\")  # <-- change if needed\n",
        "\n",
        "assert DRIVE_ROOT.exists(), f\"Path not found: {DRIVE_ROOT}\"\n",
        "print(\"Drive dataset root:\", DRIVE_ROOT)\n",
        "print(\"Folders:\", [p.name for p in DRIVE_ROOT.iterdir()])\n",
        "\n",
        "SPLIT_DRIVE = DRIVE_ROOT / \"split_dataset\"\n",
        "assert SPLIT_DRIVE.exists(), f\"split_dataset not found at {SPLIT_DRIVE}\"\n",
        "print(\"split_dataset contents:\", [p.name for p in SPLIT_DRIVE.iterdir()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEJ2hKVerSpq"
      },
      "outputs": [],
      "source": [
        "# Setup local dir path, and copy contents there\n",
        "LOCAL_ROOT = Path(\"/content/yolo_data\")\n",
        "if LOCAL_ROOT.exists():\n",
        "    shutil.rmtree(LOCAL_ROOT)\n",
        "\n",
        "print(\"Copying split_dataset to local disk...\")\n",
        "shutil.copytree(SPLIT_DRIVE, LOCAL_ROOT)\n",
        "print(\"Done. Local contents:\", [p.name for p in LOCAL_ROOT.iterdir()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrykW2YJrVMN"
      },
      "outputs": [],
      "source": [
        "# Find all the splits in each respective folder, and check its respective contents\n",
        "def find_split_dirs(root: Path):\n",
        "    # Look for train/val/test dirs\n",
        "    candidates = {p.name.lower(): p for p in root.iterdir() if p.is_dir()}\n",
        "    # Common names\n",
        "    train = candidates.get(\"train\") or candidates.get(\"training\")\n",
        "    val   = candidates.get(\"val\") or candidates.get(\"valid\") or candidates.get(\"validation\")\n",
        "    test  = candidates.get(\"test\") or candidates.get(\"testing\")\n",
        "\n",
        "    # Sometimes, split_dataset may contain images/train, labels/train directly\n",
        "    return train, val, test\n",
        "\n",
        "def find_images_labels_dir(split_dir: Path):\n",
        "    # Common patterns:\n",
        "    # split_dir/images + split_dir/labels\n",
        "    # split_dir/JPEGImages + split_dir/labels\n",
        "    # split_dir/images/train etc (if split_dir itself is root)\n",
        "    images = None\n",
        "    labels = None\n",
        "\n",
        "    for name in [\"images\", \"jpegimages\", \"jpg\", \"imgs\"]:\n",
        "        p = split_dir / name\n",
        "        if p.exists() and p.is_dir():\n",
        "            images = p\n",
        "            break\n",
        "    # VOC style inside split\n",
        "    if images is None and (split_dir / \"JPEGImages\").exists():\n",
        "        images = split_dir / \"JPEGImages\"\n",
        "\n",
        "    for name in [\"labels\", \"label\", \"yolo_labels\", \"yololabels\"]:\n",
        "        p = split_dir / name\n",
        "        if p.exists() and p.is_dir():\n",
        "            labels = p\n",
        "            break\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "train_dir, val_dir, test_dir = find_split_dirs(LOCAL_ROOT)\n",
        "\n",
        "print(\"Detected split dirs:\")\n",
        "print(\"train:\", train_dir)\n",
        "print(\"val  :\", val_dir)\n",
        "print(\"test :\", test_dir)\n",
        "\n",
        "assert train_dir is not None and val_dir is not None, \"Could not find train/val folders inside split_dataset.\"\n",
        "\n",
        "train_images, train_labels = find_images_labels_dir(train_dir)\n",
        "val_images, val_labels     = find_images_labels_dir(val_dir)\n",
        "test_images, test_labels   = (None, None)\n",
        "if test_dir:\n",
        "    test_images, test_labels = find_images_labels_dir(test_dir)\n",
        "\n",
        "print(\"\\nInside train:\")\n",
        "print(\" images:\", train_images)\n",
        "print(\" labels:\", train_labels)\n",
        "\n",
        "print(\"\\nInside val:\")\n",
        "print(\" images:\", val_images)\n",
        "print(\" labels:\", val_labels)\n",
        "\n",
        "if test_dir:\n",
        "    print(\"\\nInside test:\")\n",
        "    print(\" images:\", test_images)\n",
        "    print(\" labels:\", test_labels)\n",
        "\n",
        "assert train_images and train_labels, \"Train images/labels folders not found.\"\n",
        "assert val_images and val_labels,     \"Val images/labels folders not found.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg81Kxa1k8jH"
      },
      "source": [
        "## Counting the total number of samples + labels in each of out splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxy097MtrXYM"
      },
      "outputs": [],
      "source": [
        "def count_images(p: Path):\n",
        "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\"]\n",
        "    files = []\n",
        "    for e in exts:\n",
        "        files += list(p.glob(e))\n",
        "    return len(files)\n",
        "\n",
        "def count_labels(p: Path):\n",
        "    return len(list(p.glob(\"*.txt\")))\n",
        "\n",
        "print(\"Train images:\", count_images(train_images), \"labels:\", count_labels(train_labels))\n",
        "print(\"Val images  :\", count_images(val_images),   \"labels:\", count_labels(val_labels))\n",
        "if test_dir:\n",
        "    print(\"Test images :\", count_images(test_images), \"labels:\", count_labels(test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqq1e36ulHPE"
      },
      "source": [
        "## Getting our list of target labels (class names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD4iWHhkrXyV"
      },
      "outputs": [],
      "source": [
        "# Setup class names and contents for the YAML file\n",
        "CLASS_NAMES = [\"bud\", \"flower\", \"early fruit\", \"mid-growth\", \"mature\"]\n",
        "\n",
        "DATA_YAML = {\n",
        "    \"path\": str(LOCAL_ROOT),\n",
        "    \"train\": str(train_images.relative_to(LOCAL_ROOT)),\n",
        "    \"val\": str(val_images.relative_to(LOCAL_ROOT)),\n",
        "    \"names\": {i: name for i, name in enumerate(CLASS_NAMES)}\n",
        "}\n",
        "\n",
        "# Include test if exists\n",
        "if test_dir and test_images:\n",
        "    DATA_YAML[\"test\"] = str(test_images.relative_to(LOCAL_ROOT))\n",
        "\n",
        "yaml_path = LOCAL_ROOT / \"data.yaml\"\n",
        "with open(yaml_path, \"w\") as f:\n",
        "    yaml.safe_dump(DATA_YAML, f, sort_keys = False)\n",
        "\n",
        "print(\"Wrote:\", yaml_path)\n",
        "print(open(yaml_path).read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOV6Q3mdlQ9x"
      },
      "source": [
        "## Creating a preprocessing function to turn our YOLO bbox format into XYXY\n",
        "\n",
        " A robust preprocessing function is created to convert YOLO-normalised bounding box annotations into absolute XYXY pixel coordinates, ensuring numerical consistency and geometric correctness across the pipeline. Standardising bounding box representations is critical for accurate IoU calculation, confusion matrix construction, and precise ground-truth versus prediction comparisons during model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2FROsIaraZ8"
      },
      "outputs": [],
      "source": [
        "# Create a preprocessing function to convert YOLO bboxes to XYXY format\n",
        "def yolo_to_xyxy(xc, yc, w, h, img_w, img_h):\n",
        "    # Normalised bboxes -> pixel corners\n",
        "    x1 = (xc - w/2) * img_w\n",
        "    y1 = (yc - h/2) * img_h\n",
        "    x2 = (xc + w/2) * img_w\n",
        "    y2 = (yc + h/2) * img_h\n",
        "    return x1, y1, x2, y2\n",
        "\n",
        "def show_sample(images_dir: Path, labels_dir: Path, n=3):\n",
        "    img_files = []\n",
        "    for ext in [\"*.jpg\",\"*.jpeg\",\"*.png\"]:\n",
        "        img_files += list(images_dir.glob(ext))\n",
        "    assert len(img_files) > 0, \"No images found.\"\n",
        "    picks = random.sample(img_files, k = min(n, len(img_files)))\n",
        "\n",
        "    for img_path in picks:\n",
        "        lbl_path = labels_dir / f\"{img_path.stem}.txt\"\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        W, H = img.size\n",
        "\n",
        "        plt.figure()\n",
        "        plt.imshow(img)\n",
        "        ax = plt.gca()\n",
        "\n",
        "        if lbl_path.exists():\n",
        "            with open(lbl_path, \"r\") as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) != 5:\n",
        "                        continue\n",
        "                    cls = int(parts[0])\n",
        "                    xc, yc, bw, bh = map(float, parts[1:])\n",
        "                    x1,y1,x2,y2 = yolo_to_xyxy(xc, yc, bw, bh, W, H)\n",
        "                    ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill = False, linewidth = 2))\n",
        "                    ax.text(x1, y1, CLASS_NAMES[cls], bbox = dict(facecolor = \"white\", alpha = 0.7))\n",
        "        else:\n",
        "            ax.set_title(\"NO LABEL FILE FOUND\")\n",
        "\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "show_sample(train_images, train_labels, n = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHjWavnIl9Vy"
      },
      "source": [
        "## Loading in our model\n",
        "\n",
        "We will load the RT-DETR-L model for our use case, see the following for more about the model: https://docs.ultralytics.com/models/rtdetr/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnTmGOhGi_Ly"
      },
      "outputs": [],
      "source": [
        "# Install + load RT-DETR-L\n",
        "\n",
        "!pip -q install -U ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Fallbacks if earlier variables aren't defined\n",
        "if \"yaml_path\" not in globals():\n",
        "    yaml_path = Path(\"/content/yolo_data/data.yaml\")\n",
        "if \"HAS_TEST\" not in globals():\n",
        "    HAS_TEST = False\n",
        "\n",
        "assert Path(yaml_path).exists(), f\"data.yaml not found at: {yaml_path}\"\n",
        "\n",
        "model = YOLO(\"rtdetr-l.pt\")  # pretrained RT-DETR Large\n",
        "print(\"Loaded: RT-DETR-L\") # Corrected line\n",
        "print(\"Using data:\", yaml_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV5ZjKPRmfyl"
      },
      "source": [
        "## Training our pretrained RT-DETR-L model\n",
        "\n",
        "We fine-tune a pretrained RT-DETR-L transformer-based detector using task-specific data, optimising both classification and localisation performance. Transfer learning is employed to reduce training time, improve convergence stability, and maximize performance under practical compute constraints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZ4B8_3tjFVA"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "\n",
        "results = model.train(\n",
        "    data = str(yaml_path),\n",
        "    epochs = 120,     # Bump to 150-200 if you want maximum mAP\n",
        "    imgsz = 960,      # 640 faster; 960 often boosts mAP for small objects\n",
        "    batch = 8,        # If OOM -> 4, 2, or 1\n",
        "    device = 0,\n",
        "    workers = 2,\n",
        "    project = \"/content/runs\",\n",
        "    name = \"rtdetr_l_training\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzjmO447nA_s"
      },
      "source": [
        "## Evaluating our model on the test set\n",
        "\n",
        "Evaluating the model is just as important as training it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-kd_GcpjHPx"
      },
      "outputs": [],
      "source": [
        "# Evaluate on val (and test if present)\n",
        "\n",
        "val_metrics = model.val(data = str(yaml_path))\n",
        "print(\"Val metrics object:\", val_metrics)\n",
        "\n",
        "# Test split eval (only if your data.yaml contains \"test:\")\n",
        "if HAS_TEST:\n",
        "    test_metrics = model.val(data = str(yaml_path), split = \"test\")\n",
        "    print(\"Test metrics object:\", test_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAsYxe9TjKUj"
      },
      "outputs": [],
      "source": [
        "# Predict samples + save artifacts back to Drive\n",
        "\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Try to infer run directory\n",
        "RUN_DIR = Path(\"/content/runs/detect/rtdetr_l_training\")\n",
        "WEIGHTS_DIR = RUN_DIR / \"weights\"\n",
        "best = WEIGHTS_DIR / \"best.pt\"\n",
        "last = WEIGHTS_DIR / \"last.pt\"\n",
        "\n",
        "print(\"Run dir:\", RUN_DIR)\n",
        "print(\"Best exists:\", best.exists(), \"|\", best)\n",
        "print(\"Last exists:\", last.exists(), \"|\", last)\n",
        "\n",
        "# Pick some sample images for inference\n",
        "# If you already have test_images/val_images variables, we’ll use them\n",
        "sample_dir = None\n",
        "if HAS_TEST and \"test_images\" in globals() and test_images is not None:\n",
        "    sample_dir = Path(test_images)\n",
        "elif \"val_images\" in globals() and val_images is not None:\n",
        "    sample_dir = Path(val_images)\n",
        "else:\n",
        "    # Fallback: Use the train folder from yaml_path \"path\"+\"val\"\n",
        "    # (this is a best-effort fallback; earlier stages usually define val_images)\n",
        "    sample_dir = RUN_DIR  # Won't work unless you set val_images/test_images\n",
        "    print(\"Note: sample_dir fallback may not point to images. Define val_images/test_images for clean inference.\")\n",
        "\n",
        "if sample_dir and sample_dir.exists():\n",
        "    imgs = []\n",
        "    for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\"):\n",
        "        imgs += list(sample_dir.glob(ext))\n",
        "    imgs = sorted(imgs)\n",
        "    if imgs:\n",
        "        picks = random.sample(imgs, k = min(10, len(imgs)))\n",
        "        _ = model.predict(picks, imgsz = 960, conf = 0.25, save = True)\n",
        "        print(\"Predictions saved under:\", Path(\"/content/runs/detect\").resolve())\n",
        "    else:\n",
        "        print(\"No images found for inference in:\", sample_dir)\n",
        "\n",
        "# Save to Drive\n",
        "DRIVE_SAVE = Path(\"/content/drive/MyDrive/rtdetr_l_outputs\")\n",
        "DRIVE_SAVE.mkdir(parents = True, exist_ok = True)\n",
        "\n",
        "# Save weights + data.yaml\n",
        "if best.exists():\n",
        "    shutil.copy2(best, DRIVE_SAVE / \"best.pt\")\n",
        "if last.exists():\n",
        "    shutil.copy2(last, DRIVE_SAVE / \"last.pt\")\n",
        "shutil.copy2(Path(yaml_path), DRIVE_SAVE / \"data.yaml\")\n",
        "\n",
        "# Save key training artifacts if present\n",
        "for f in [\"results.csv\", \"results.png\", \"confusion_matrix.png\", \"F1_curve.png\", \"PR_curve.png\"]:\n",
        "    p = RUN_DIR / f\n",
        "    if p.exists():\n",
        "        shutil.copy2(p, DRIVE_SAVE / p.name)\n",
        "\n",
        "print(\"Saved to Drive:\", DRIVE_SAVE)\n",
        "print(\"Files:\", [p.name for p in DRIVE_SAVE.iterdir()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STQgol3XnirR"
      },
      "source": [
        "## Inspecting ground-truth bboxes vs our model's\n",
        "\n",
        "Visually compare ground-truth and predicted bounding boxes to assess localisation accuracy and identify qualitative failure modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhIIAwKQV_aY"
      },
      "outputs": [],
      "source": [
        "# Compare Ground Truth vs RT-DETR-L Predictions (side-by-side)\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup hyperparameters\n",
        "NUM_SAMPLES = 5\n",
        "CONF_THR = 0.25\n",
        "\n",
        "# Pick dataset split\n",
        "if \"HAS_TEST\" in globals() and HAS_TEST and \"test_images\" in globals():\n",
        "    IMG_DIR = Path(test_images)\n",
        "    LAB_DIR = Path(test_labels)\n",
        "else:\n",
        "    IMG_DIR = Path(val_images)\n",
        "    LAB_DIR = Path(val_labels)\n",
        "\n",
        "assert IMG_DIR.exists() and LAB_DIR.exists(), \"Image/label dirs not found\"\n",
        "\n",
        "def yolo_to_xyxy(xc, yc, w, h, W, H):\n",
        "    x1 = (xc - w/2) * W\n",
        "    y1 = (yc - h/2) * H\n",
        "    x2 = (xc + w/2) * W\n",
        "    y2 = (yc + h/2) * H\n",
        "    return x1, y1, x2, y2\n",
        "\n",
        "def draw_gt(ax, img_path, label_path):\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    W, H = img.size\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(\"Ground Truth\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    if label_path.exists():\n",
        "        with open(label_path) as f:\n",
        "            for line in f:\n",
        "                cls, xc, yc, bw, bh = map(float, line.split())\n",
        "                cls = int(cls)\n",
        "                x1, y1, x2, y2 = yolo_to_xyxy(xc, yc, bw, bh, W, H)\n",
        "                ax.add_patch(\n",
        "                    plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                                  fill = False, linewidth=2)\n",
        "                )\n",
        "                ax.text(\n",
        "                    x1, y1, CLASS_NAMES[cls],\n",
        "                    bbox = dict(facecolor = \"white\", alpha = 0.7),\n",
        "                    fontsize = 9\n",
        "                )\n",
        "\n",
        "def draw_pred(ax, img_path):\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(\"RT-DETR-L Prediction\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    result = model.predict(img_path, conf = CONF_THR, imgsz = 960, verbose = False)[0]\n",
        "\n",
        "    if result.boxes is not None:\n",
        "        boxes = result.boxes.xyxy.cpu().numpy()\n",
        "        scores = result.boxes.conf.cpu().numpy()\n",
        "        clss = result.boxes.cls.cpu().numpy().astype(int)\n",
        "\n",
        "        for (x1, y1, x2, y2), sc, cls in zip(boxes, scores, clss):\n",
        "            ax.add_patch(\n",
        "                plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
        "                              fill=False, linewidth=2)\n",
        "            )\n",
        "            ax.text(\n",
        "                x1, y1,\n",
        "                f\"{CLASS_NAMES[cls]} {sc:.2f}\",\n",
        "                bbox = dict(facecolor = \"yellow\", alpha = 0.7),\n",
        "                fontsize = 9\n",
        "            )\n",
        "\n",
        "img_files = []\n",
        "for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\"):\n",
        "    img_files += list(IMG_DIR.glob(ext))\n",
        "\n",
        "assert img_files, \"No images found\"\n",
        "samples = random.sample(img_files, k = min(NUM_SAMPLES, len(img_files)))\n",
        "\n",
        "for img_path in samples:\n",
        "    lbl_path = LAB_DIR / f\"{img_path.stem}.txt\"\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize = (14, 6))\n",
        "    draw_gt(axes[0], img_path, lbl_path)\n",
        "    draw_pred(axes[1], img_path)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMOyCZ1PoP2R"
      },
      "source": [
        "## Visualising other metrics as well\n",
        "\n",
        "Other visualisations of extended evaluation metrics — including per-class precision and recall, IoU distributions, and confusion-based error patterns — to enable detailed diagnostic analysis of model behaviour. By exposing class-level performance trends and failure characteristics, these visualizations support informed engineering decisions around model tuning, robustness, and deployment readiness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olG0tVK2V_zq"
      },
      "outputs": [],
      "source": [
        "# Per-class Precision / Recall for RT-DETR-L vs YOLO Ground Truth (IoU matching)\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setup required hyperparameters\n",
        "CONF_THR = 0.25     # Prediction confidence threshold\n",
        "IOU_THR  = 0.50     # IoU threshold for a \"match\"\n",
        "MAX_IMAGES = None   # Set to an int (e.g., 500) to evaluate faster; None = all images\n",
        "IMG_SIZE = 960      # Should match training/eval size (or 640 if you trained with 640)\n",
        "\n",
        "# Choose split: test if available, else val\n",
        "if \"HAS_TEST\" in globals() and HAS_TEST and \"test_images\" in globals() and test_images is not None:\n",
        "    IMG_DIR = Path(test_images)\n",
        "    LAB_DIR = Path(test_labels)\n",
        "    SPLIT_NAME = \"test\"\n",
        "else:\n",
        "    IMG_DIR = Path(val_images)\n",
        "    LAB_DIR = Path(val_labels)\n",
        "    SPLIT_NAME = \"val\"\n",
        "\n",
        "assert IMG_DIR.exists() and LAB_DIR.exists(), \"Image/label dirs not found.\"\n",
        "assert \"CLASS_NAMES\" in globals(), \"CLASS_NAMES not defined.\"\n",
        "num_classes = len(CLASS_NAMES)\n",
        "\n",
        "def yolo_txt_to_xyxy(label_path: Path, W: int, H: int):\n",
        "    \"\"\"Read YOLO labels and return list of (cls, [x1,y1,x2,y2]) in pixel coords.\"\"\"\n",
        "    gts = []\n",
        "    if not label_path.exists():\n",
        "        return gts\n",
        "    with open(label_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                continue\n",
        "            cls = int(float(parts[0]))\n",
        "            xc, yc, bw, bh = map(float, parts[1:])\n",
        "            x1 = (xc - bw/2) * W\n",
        "            y1 = (yc - bh/2) * H\n",
        "            x2 = (xc + bw/2) * W\n",
        "            y2 = (yc + bh/2) * H\n",
        "            # clamp\n",
        "            x1, y1 = max(0.0, x1), max(0.0, y1)\n",
        "            x2, y2 = min(float(W-1), x2), min(float(H-1), y2)\n",
        "            if x2 > x1 and y2 > y1:\n",
        "                gts.append((cls, np.array([x1, y1, x2, y2], dtype = np.float32)))\n",
        "    return gts\n",
        "\n",
        "def iou_xyxy(a, b):\n",
        "    \"\"\"IoU for two [x1,y1,x2,y2] boxes.\"\"\"\n",
        "    xA = max(a[0], b[0]); yA = max(a[1], b[1])\n",
        "    xB = min(a[2], b[2]); yB = min(a[3], b[3])\n",
        "    inter_w = max(0.0, xB - xA)\n",
        "    inter_h = max(0.0, yB - yA)\n",
        "    inter = inter_w * inter_h\n",
        "    area_a = max(0.0, a[2]-a[0]) * max(0.0, a[3]-a[1])\n",
        "    area_b = max(0.0, b[2]-b[0]) * max(0.0, b[3]-b[1])\n",
        "    union = area_a + area_b - inter\n",
        "    return 0.0 if union <= 0 else float(inter / union)\n",
        "\n",
        "def match_class_greedy(gt_boxes, pred_boxes, iou_thr):\n",
        "    \"\"\"\n",
        "    Greedy matching for one class:\n",
        "    gt_boxes: list of boxes\n",
        "    pred_boxes: list of boxes\n",
        "    returns TP, FP, FN\n",
        "    \"\"\"\n",
        "    if len(gt_boxes) == 0 and len(pred_boxes) == 0:\n",
        "        return 0, 0, 0\n",
        "    if len(gt_boxes) == 0:\n",
        "        return 0, len(pred_boxes), 0\n",
        "    if len(pred_boxes) == 0:\n",
        "        return 0, 0, len(gt_boxes)\n",
        "\n",
        "    used_gt = [False] * len(gt_boxes)\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "\n",
        "    # If you want \"best practice\", sort predictions by confidence before matching.\n",
        "    # Here, pred_boxes is already filtered; we'll assume it was confidence-sorted upstream when created.\n",
        "    for pb in pred_boxes:\n",
        "        best_iou = 0.0\n",
        "        best_j = -1\n",
        "        for j, gb in enumerate(gt_boxes):\n",
        "            if used_gt[j]:\n",
        "                continue\n",
        "            i = iou_xyxy(pb, gb)\n",
        "            if i > best_iou:\n",
        "                best_iou = i\n",
        "                best_j = j\n",
        "        if best_iou >= iou_thr and best_j >= 0:\n",
        "            tp += 1\n",
        "            used_gt[best_j] = True\n",
        "        else:\n",
        "            fp += 1\n",
        "\n",
        "    fn = used_gt.count(False)\n",
        "    return tp, fp, fn\n",
        "\n",
        "# Collect necessary files\n",
        "img_files = []\n",
        "for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\"):\n",
        "    img_files += list(IMG_DIR.glob(ext))\n",
        "img_files = sorted(img_files)\n",
        "\n",
        "if MAX_IMAGES is not None:\n",
        "    img_files = img_files[:MAX_IMAGES]\n",
        "\n",
        "assert len(img_files) > 0, f\"No images found in {IMG_DIR}\"\n",
        "\n",
        "print(f\"Evaluating split: {SPLIT_NAME} | Images: {len(img_files)} | CONF_THR={CONF_THR} | IOU_THR={IOU_THR}\")\n",
        "\n",
        "# Counters per class\n",
        "TP = np.zeros(num_classes, dtype = np.int64)\n",
        "FP = np.zeros(num_classes, dtype = np.int64)\n",
        "FN = np.zeros(num_classes, dtype = np.int64)\n",
        "\n",
        "for idx, img_path in enumerate(img_files):\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    W, H = img.size\n",
        "    lbl_path = LAB_DIR / f\"{img_path.stem}.txt\"\n",
        "\n",
        "    # Ground truth per class\n",
        "    gt_all = yolo_txt_to_xyxy(lbl_path, W, H)\n",
        "    gt_by_class = [[] for _ in range(num_classes)]\n",
        "    for cls, box in gt_all:\n",
        "        if 0 <= cls < num_classes:\n",
        "            gt_by_class[cls].append(box)\n",
        "\n",
        "    # Predictions (Ultralytics)\n",
        "    r = model.predict(str(img_path), conf = CONF_THR, imgsz = IMG_SIZE, verbose = False)[0]\n",
        "    pred_by_class = [[] for _ in range(num_classes)]\n",
        "    if r.boxes is not None and len(r.boxes) > 0:\n",
        "        boxes = r.boxes.xyxy.cpu().numpy()\n",
        "        confs = r.boxes.conf.cpu().numpy()\n",
        "        clss  = r.boxes.cls.cpu().numpy().astype(int)\n",
        "\n",
        "        # Sort preds by confidence descending (important for greedy matching)\n",
        "        order = np.argsort(-confs)\n",
        "        boxes, confs, clss = boxes[order], confs[order], clss[order]\n",
        "\n",
        "        for b, c in zip(boxes, clss):\n",
        "            if 0 <= c < num_classes:\n",
        "                pred_by_class[c].append(b.astype(np.float32))\n",
        "\n",
        "    # Match per class\n",
        "    for c in range(num_classes):\n",
        "        tp, fp, fn = match_class_greedy(gt_by_class[c], pred_by_class[c], IOU_THR)\n",
        "        TP[c] += tp\n",
        "        FP[c] += fp\n",
        "        FN[c] += fn\n",
        "\n",
        "# Calculate respective metrics\n",
        "precision = TP / np.maximum(TP + FP, 1)\n",
        "recall    = TP / np.maximum(TP + FN, 1)\n",
        "f1        = 2 * precision * recall / np.maximum(precision + recall, 1e-12)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"class\": CLASS_NAMES,\n",
        "    \"TP\": TP,\n",
        "    \"FP\": FP,\n",
        "    \"FN\": FN,\n",
        "    \"precision\": precision,\n",
        "    \"recall\": recall,\n",
        "    \"f1\": f1\n",
        "})\n",
        "\n",
        "# Macro averages\n",
        "macro = df[[\"precision\", \"recall\", \"f1\"]].mean()\n",
        "print(\"\\nPer-class metrics:\")\n",
        "display(df)\n",
        "\n",
        "print(\"\\nMacro avg:\")\n",
        "display(macro)\n",
        "\n",
        "# Plot the DataFrame + graphs\n",
        "plt.figure(figsize=(10,5))\n",
        "x = np.arange(num_classes)\n",
        "plt.bar(x - 0.2, df[\"precision\"].values, width = 0.4, label = \"Precision\")\n",
        "plt.bar(x + 0.2, df[\"recall\"].values, width = 0.4, label = \"Recall\")\n",
        "plt.xticks(x, CLASS_NAMES, rotation = 30, ha = \"right\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.title(f\"Per-class Precision/Recall ({SPLIT_NAME}) @ IoU={IOU_THR}, conf={CONF_THR}\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I4m-epIXh1A"
      },
      "outputs": [],
      "source": [
        "# Enhanced Detection Analysis: Normalised Confusion Matrix + Per-class IoU + Error Breakdown\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "# Setup hyperparameters\n",
        "CONF_THR = 0.25\n",
        "IOU_THR  = 0.50\n",
        "IMG_SIZE = 960        # Use same as training/eval (640 or 960)\n",
        "MAX_IMAGES = None     # Set e.g. 500 for faster debug; None = all\n",
        "\n",
        "# Choose split\n",
        "if \"HAS_TEST\" in globals() and HAS_TEST and \"test_images\" in globals() and test_images is not None:\n",
        "    IMG_DIR = Path(test_images)\n",
        "    LAB_DIR = Path(test_labels)\n",
        "    SPLIT = \"test\"\n",
        "else:\n",
        "    IMG_DIR = Path(val_images)\n",
        "    LAB_DIR = Path(val_labels)\n",
        "    SPLIT = \"val\"\n",
        "\n",
        "assert IMG_DIR.exists() and LAB_DIR.exists(), \"Image/label dirs not found.\"\n",
        "assert \"CLASS_NAMES\" in globals(), \"CLASS_NAMES not defined.\"\n",
        "assert \"model\" in globals(), \"model not loaded.\"\n",
        "\n",
        "num_classes = len(CLASS_NAMES)\n",
        "BG = num_classes  # background index\n",
        "labels_cm = CLASS_NAMES + [\"background\"]\n",
        "\n",
        "def yolo_to_xyxy(xc, yc, w, h, W, H):\n",
        "    x1 = (xc - w/2) * W\n",
        "    y1 = (yc - h/2) * H\n",
        "    x2 = (xc + w/2) * W\n",
        "    y2 = (yc + h/2) * H\n",
        "    # clamp\n",
        "    x1 = max(0.0, min(x1, W-1.0))\n",
        "    y1 = max(0.0, min(y1, H-1.0))\n",
        "    x2 = max(0.0, min(x2, W-1.0))\n",
        "    y2 = max(0.0, min(y2, H-1.0))\n",
        "    if x2 <= x1 or y2 <= y1:\n",
        "        return None\n",
        "    return np.array([x1, y1, x2, y2], dtype = np.float32)\n",
        "\n",
        "def iou_xyxy(a, b):\n",
        "    xA = max(a[0], b[0]); yA = max(a[1], b[1])\n",
        "    xB = min(a[2], b[2]); yB = min(a[3], b[3])\n",
        "    inter = max(0.0, xB-xA) * max(0.0, yB-yA)\n",
        "    area_a = max(0.0, a[2]-a[0]) * max(0.0, a[3]-a[1])\n",
        "    area_b = max(0.0, b[2]-b[0]) * max(0.0, b[3]-b[1])\n",
        "    union = area_a + area_b - inter\n",
        "    return 0.0 if union <= 0 else float(inter / union)\n",
        "\n",
        "# Calculate metrics for confusion matrix\n",
        "cm = np.zeros((num_classes + 1, num_classes + 1), dtype=np.int64)\n",
        "\n",
        "TP = np.zeros(num_classes, dtype = np.int64)\n",
        "FP = np.zeros(num_classes, dtype = np.int64)\n",
        "FN = np.zeros(num_classes, dtype = np.int64)\n",
        "\n",
        "# IoU stats for true positives per class\n",
        "iou_sum = np.zeros(num_classes, dtype = np.float64)\n",
        "iou_cnt = np.zeros(num_classes, dtype = np.int64)\n",
        "\n",
        "# Get the list of images\n",
        "img_files = []\n",
        "for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\"):\n",
        "    img_files += list(IMG_DIR.glob(ext))\n",
        "img_files = sorted(img_files)\n",
        "\n",
        "if MAX_IMAGES is not None:\n",
        "    img_files = img_files[:MAX_IMAGES]\n",
        "\n",
        "assert len(img_files) > 0, f\"No images found in {IMG_DIR}\"\n",
        "\n",
        "print(f\"Running enhanced analysis on {len(img_files)} images ({SPLIT}) | conf={CONF_THR} | IoU={IOU_THR}\")\n",
        "\n",
        "for img_path in img_files:\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    W, H = img.size\n",
        "    lbl_path = LAB_DIR / f\"{img_path.stem}.txt\"\n",
        "\n",
        "    # Load GT\n",
        "    gt = []\n",
        "    if lbl_path.exists():\n",
        "        with open(lbl_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) != 5:\n",
        "                    continue\n",
        "                cls = int(float(parts[0]))\n",
        "                xc, yc, bw, bh = map(float, parts[1:])\n",
        "                box = yolo_to_xyxy(xc, yc, bw, bh, W, H)\n",
        "                if box is None:\n",
        "                    continue\n",
        "                if 0 <= cls < num_classes:\n",
        "                    gt.append((cls, box))\n",
        "\n",
        "    gt_used = [False] * len(gt)\n",
        "\n",
        "    # Calculate model preds\n",
        "    r = model.predict(str(img_path), conf = CONF_THR, imgsz = IMG_SIZE, verbose = False)[0]\n",
        "    preds = []\n",
        "    if r.boxes is not None and len(r.boxes) > 0:\n",
        "        p_boxes = r.boxes.xyxy.cpu().numpy()\n",
        "        p_confs = r.boxes.conf.cpu().numpy()\n",
        "        p_cls   = r.boxes.cls.cpu().numpy().astype(int)\n",
        "\n",
        "        order = np.argsort(-p_confs)  # Sort description by confidence for greedy matching\n",
        "        for i in order:\n",
        "            c = int(p_cls[i])\n",
        "            if 0 <= c < num_classes:\n",
        "                preds.append((c, p_boxes[i].astype(np.float32), float(p_confs[i])))\n",
        "\n",
        "    # Greedy match each pred to best unused GT (any class)\n",
        "    for p_c, p_b, _ in preds:\n",
        "        best_iou = 0.0\n",
        "        best_j = -1\n",
        "        for j, (g_c, g_b) in enumerate(gt):\n",
        "            if gt_used[j]:\n",
        "                continue\n",
        "            val = iou_xyxy(p_b, g_b)\n",
        "            if val > best_iou:\n",
        "                best_iou = val\n",
        "                best_j = j\n",
        "\n",
        "        if best_iou >= IOU_THR and best_j >= 0:\n",
        "            g_c = gt[best_j][0]\n",
        "            # Confusion matrix entry: GT -> Pred\n",
        "            cm[g_c, p_c] += 1\n",
        "\n",
        "            # Update TP/FP/FN + IoU stats\n",
        "            if p_c == g_c:\n",
        "                TP[g_c] += 1\n",
        "                iou_sum[g_c] += best_iou\n",
        "                iou_cnt[g_c] += 1\n",
        "            else:\n",
        "                # Misclassification: counts as FP for predicted class, FN for GT class\n",
        "                FP[p_c] += 1\n",
        "                FN[g_c] += 1\n",
        "\n",
        "            gt_used[best_j] = True\n",
        "        else:\n",
        "            # Unmatched prediction -> false positive (background is GT)\n",
        "            cm[BG, p_c] += 1\n",
        "            FP[p_c] += 1\n",
        "\n",
        "    # Unmatched GT -> false negative (pred background)\n",
        "    for used, (g_c, _) in zip(gt_used, gt):\n",
        "        if not used:\n",
        "            cm[g_c, BG] += 1\n",
        "            FN[g_c] += 1\n",
        "\n",
        "# Normalise our confusion matrix be between 0-1\n",
        "# Row-normalised by GT (each row sums to 1 if row has samples)\n",
        "row_sums = cm.sum(axis = 1, keepdims = True).astype(np.float64)\n",
        "cm_norm = np.divide(cm, np.maximum(row_sums, 1.0))\n",
        "\n",
        "# Calculate the IOU per class\n",
        "iou_avg = np.divide(iou_sum, np.maximum(iou_cnt, 1))\n",
        "\n",
        "# Create error breakdown table\n",
        "precision = TP / np.maximum(TP + FP, 1)\n",
        "recall    = TP / np.maximum(TP + FN, 1)\n",
        "f1        = 2 * precision * recall / np.maximum(precision + recall, 1e-12)\n",
        "\n",
        "# Top confusion for each GT class (excluding diagonal & background column)\n",
        "top_confusions = []\n",
        "for g in range(num_classes):\n",
        "    row = cm[g, :num_classes].copy()\n",
        "    row[g] = 0  # exclude correct\n",
        "    if row.sum() == 0:\n",
        "        top_confusions.append(\"-\")\n",
        "    else:\n",
        "        p = int(np.argmax(row))\n",
        "        top_confusions.append(f\"{CLASS_NAMES[g]} → {CLASS_NAMES[p]} ({row[p]})\")\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"class\": CLASS_NAMES,\n",
        "    \"TP\": TP,\n",
        "    \"FP\": FP,\n",
        "    \"FN\": FN,\n",
        "    \"precision\": precision,\n",
        "    \"recall\": recall,\n",
        "    \"f1\": f1,\n",
        "    \"avg_iou_TP\": iou_avg,\n",
        "    \"top_confusion\": top_confusions\n",
        "})\n",
        "\n",
        "print(\"\\n Class-wise error breakdown\")\n",
        "display(df)\n",
        "\n",
        "print(\"\\nMacro averages:\")\n",
        "display(df[[\"precision\",\"recall\",\"f1\",\"avg_iou_TP\"]].mean())\n",
        "\n",
        "# Plot individual displays\n",
        "# 1) Raw confusion matrix\n",
        "plt.figure(figsize = (9, 7))\n",
        "plt.imshow(cm, aspect = \"auto\")\n",
        "plt.title(f\"Confusion Matrix (counts) | {SPLIT} | IoU={IOU_THR} conf={CONF_THR}\")\n",
        "plt.xticks(range(num_classes + 1), labels_cm, rotation = 30, ha = \"right\")\n",
        "plt.yticks(range(num_classes + 1), labels_cm)\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2) Normalized confusion matrix (row-normalized by GT)\n",
        "plt.figure(figsize = (9, 7))\n",
        "plt.imshow(cm_norm, aspect = \"auto\", vmin = 0, vmax = 1)\n",
        "plt.title(f\"Confusion Matrix (row-normalized by GT) | {SPLIT} | IoU={IOU_THR} conf={CONF_THR}\")\n",
        "plt.xticks(range(num_classes+1), labels_cm, rotation = 30, ha = \"right\")\n",
        "plt.yticks(range(num_classes+1), labels_cm)\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3) Per-class avg IoU bar chart\n",
        "plt.figure(figsize = (9, 4))\n",
        "plt.bar(range(num_classes), iou_avg)\n",
        "plt.xticks(range(num_classes), CLASS_NAMES, rotation = 30, ha = \"right\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.title(f\"Per-class Average IoU on True Positives | {SPLIT}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q00T08yrbJFb"
      },
      "outputs": [],
      "source": [
        "# Ultralytics automatically saves loss curves here:\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Corrected RUN_DIR to match the actual training output path\n",
        "RUN_DIR = Path(\"/content/runs/rtdetr_l_training\")\n",
        "\n",
        "img = Image.open(RUN_DIR / \"results.png\")\n",
        "plt.figure(figsize = (8,6))\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training & Validation Loss Curves\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edyglo7cbLrh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "H100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}